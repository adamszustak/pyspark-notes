{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f212dab-d62a-4898-8277-c44aea7a7aad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,expr,to_timestamp,when, desc, asc, concat, lit, count, avg, udf\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "### Set up Spark UI ###\n",
    "### Settings conf can be set here or dynamically at runtime (spark-submit)\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"App\").\\\n",
    "config(\"spark.sql.catalogImplementation\", \"hive\").\\\n",
    "config(\"spark.driver.bindAddress\",\"localhost\").\\\n",
    "config(\"spark.ui.port\",\"4050\").\\\n",
    "enableHiveSupport().\\\n",
    "getOrCreate()\n",
    "\n",
    "### Set up wo Spark UI ###\n",
    "# spark=SparkSession.builder.appName('App').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46fc1b10-4003-4c51-859e-dcbeb9fe1e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.12:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>App</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x117958f40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ddb4ac-b291-4128-96c7-5c2bbefcc7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[date: int, delay: int, distance: int, origin: string, destination: string]\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             delay|\n",
      "+-------+------------------+\n",
      "|  count|           1391578|\n",
      "|   mean|12.079802928761449|\n",
      "| stddev| 38.80773374985683|\n",
      "|    min|              -112|\n",
      "|    max|              1642|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Read file with schema infer ### - 2 options\n",
    "# df_pyspark=spark.read.option('header','true').csv('flight_delays/data_flights.csv',inferSchema=True)\n",
    "# df_pyspark=spark.read.csv('flight_delays/data_flights.csv',header=True,inferSchema=True)\n",
    "\n",
    "### Read file with schema predefined - recomm ###\n",
    "schema = \"`date` INT, `delay` INT, `distance` INT, `origin` STRING, `destination` STRING\"\n",
    "df_pyspark=spark.read.csv('flight_delays/data_flights.csv',header=True,schema=schema, sep=',')\n",
    "\n",
    "print(df_pyspark)\n",
    "print(type(df_pyspark))\n",
    "\n",
    "### Print basic statistics (min, max, count, mean, stddev)\n",
    "df_pyspark.describe([\"delay\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "413a2b2e-5b14-49d2-b617-c87a3486cf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(date=1011245, delay=6, distance=602, origin='ABE', destination='ATL'), Row(date=1020600, delay=-8, distance=369, origin='ABE', destination='DTW'), Row(date=1021245, delay=-2, distance=602, origin='ABE', destination='ATL')]\n",
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### They are all actions()\n",
    "### Show top 3 rows as Row class - the same goes with TAKE, FIRST, can be used to create new DF\n",
    "print(df_pyspark.head(3))\n",
    "\n",
    "### Show top 3 rows as table - no return\n",
    "print(df_pyspark.show(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc9c124f-2c10-4087-b123-d68d5a20595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'delay', 'distance', 'origin', 'destination']\n",
      "root\n",
      " |-- date: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n",
      "None\n",
      "[('date', 'int'), ('delay', 'int'), ('distance', 'int'), ('origin', 'string'), ('destination', 'string')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructType([StructField('date', IntegerType(), True), StructField('delay', IntegerType(), True), StructField('distance', IntegerType(), True), StructField('origin', StringType(), True), StructField('destination', StringType(), True)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Print column names as list\n",
    "print(df_pyspark.columns)\n",
    "\n",
    "### Print readable schema\n",
    "print(df_pyspark.printSchema())\n",
    "\n",
    "### Print cols and datatypes\n",
    "print(df_pyspark.dtypes)\n",
    "\n",
    "### Print schema that can be reused\n",
    "df_pyspark.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0858fece-08fb-4697-b9a9-e9356a6b92fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|delay|(delay + 10)|\n",
      "+-----+------------+\n",
      "|    6|          16|\n",
      "|   -8|           2|\n",
      "+-----+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+------------+\n",
      "|delay|(delay + 10)|\n",
      "+-----+------------+\n",
      "|    6|          16|\n",
      "|   -8|           2|\n",
      "+-----+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "-RECORD 0------\n",
      " delay   | 6   \n",
      " new_del | 16  \n",
      "-RECORD 1------\n",
      " delay   | -8  \n",
      " new_del | 2   \n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+-------+\n",
      "|delay|new_del|\n",
      "+-----+-------+\n",
      "|    6|     16|\n",
      "|   -8|      2|\n",
      "+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----+\n",
      "|delay|\n",
      "+-----+\n",
      "|    6|\n",
      "|   -8|\n",
      "+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Select specific cols, compare diff options to work with cols\n",
    "df_pyspark.select(\"delay\").show(2)\n",
    "df_pyspark.select(df_pyspark.delay, (df_pyspark[\"delay\"] + 10)).show(2)\n",
    "df_pyspark.select(col(\"delay\"), (col(\"delay\") + 10)).show(2)\n",
    "df_pyspark.select(expr(\"delay\"), expr(\"delay+10\").alias(\"new_del\")).show(2, vertical=True)\n",
    "df_pyspark.selectExpr(\"delay\", \"delay+10 AS new_del\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c46808-7c61-4f58-b60e-a416d81ac750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+-------+\n",
      "|   date|delay|distance|origin|destination|  route|\n",
      "+-------+-----+--------+------+-----------+-------+\n",
      "|1011245|    6|     602|   ABE|        ATL|ABE-ATL|\n",
      "+-------+-----+--------+------+-----------+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "None\n",
      "['new_date', 'delay', 'distance', 'origin', 'destination', 'route']\n",
      "[('date', 'int'), ('delay', 'float'), ('distance', 'int'), ('origin', 'string'), ('destination', 'string'), ('route', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+-------------+\n",
      "|   date|delay|distance|origin|destination|Flight_Delays|\n",
      "+-------+-----+--------+------+-----------+-------------+\n",
      "|3141155|  -19|     139|   YUM|        PHX| Short Delays|\n",
      "|2161859|  -17|     139|   YUM|        PHX| Short Delays|\n",
      "+-------+-----+--------+------+-----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Add or replace column\n",
    "new_df = df_pyspark.withColumn(\"route\", concat(col(\"origin\"), lit(\"-\"), col(\"destination\")))\n",
    "print(new_df.show(1))\n",
    "\n",
    "### Rename column\n",
    "print(new_df.withColumnRenamed(\"date\",\"new_date\").columns)\n",
    "\n",
    "### Change type column\n",
    "print(new_df.withColumn(\"delay\", new_df.delay.cast(\"Float\").alias(\"new_del\")).dtypes)\n",
    "\n",
    "### Delete column\n",
    "new_df.drop(\"date\").columns\n",
    "\n",
    "### When statement\n",
    "df_pyspark.withColumn(\"Flight_Delays\",when(col(\"delay\") > 360, 'Very Long Delays').when((col(\"delay\") > 120) & (col(\"delay\") < 360), 'Long Delays').otherwise('Short Delays')).orderBy(desc(\"origin\"), asc(\"delay\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd4af5-da91-4d1f-bc90-12b333947b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cast columns as timestamp\n",
    "new_df.withColumn(\"parsed_date\", to_timestamp(col(\"date\"), \"MMddyyyy\")).drop(\"date\")\n",
    "new_df.select(year(\"parsed_date\"), month(\"parsed_date\"), day(\"parsed_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b04495f-7584-43b6-897c-0655ab402a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Get only 10 rows\n",
    "df_small = df_pyspark.limit(10)\n",
    "\n",
    "### Create in-memory DF and merge two DFs\n",
    "new_row = spark.createDataFrame([(1011244,5,7, None, None), (None,None,None, None, None)], schema=df_small.schema)\n",
    "df_null = df_small.union(new_row)\n",
    "\n",
    "### Count rows\n",
    "df_null.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb079e4b-aff3-4f24-8539-d24e51a8d5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|delay|   date|\n",
      "+-----+-------+\n",
      "|    6|1011245|\n",
      "|   10|1041243|\n",
      "+-----+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Filtering rows\n",
    "df_small.filter(\"delay>=6\").select('delay', 'date').show(2)\n",
    "\n",
    "### two AND conditions\n",
    "df_small.filter((df_small[\"delay\"]>=6) & (df_small[\"date\"]<1051245)).show(2)\n",
    "\n",
    "### two OR and NOT operator\n",
    "df_small.filter(~(df_small.delay>=6) | (df_small.date<1051245)).show(2)\n",
    "\n",
    "### Alias for filter, show NOT NULL\n",
    "df_small.where((col(\"delay\") >= 6) & (col(\"delay\").isNotNull())).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab14177e-ec73-4ab0-8741-593ffffadbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----+\n",
      "|origin|destination|count|\n",
      "+------+-----------+-----+\n",
      "|   ABE|        ATL|    9|\n",
      "|   ABE|        DTW|    1|\n",
      "+------+-----------+-----+\n",
      "\n",
      "+------+-----------+-----+------------------+\n",
      "|origin|destination|count|        avg(delay)|\n",
      "+------+-----------+-----+------------------+\n",
      "|   ABE|        ATL|    9|14.555555555555555|\n",
      "|  NULL|       NULL|    2|               5.0|\n",
      "|   ABE|        DTW|    1|              -8.0|\n",
      "+------+-----------+-----+------------------+\n",
      "\n",
      "+------+-----------+-----+------------------+\n",
      "|origin|destination|count|        avg(delay)|\n",
      "+------+-----------+-----+------------------+\n",
      "|   ABE|        ATL|    9|14.555555555555555|\n",
      "|  NULL|       NULL|    2|               5.0|\n",
      "|   ABE|        DTW|    1|              -8.0|\n",
      "+------+-----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Aggr, count rows and order, can be also mean, max, sum....\n",
    "df_null.select(\"*\").where(col(\"origin\").isNotNull()).groupBy(\"origin\", \"destination\").count().orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "### More than one aggr\n",
    "df_null.groupBy(\"origin\", \"destination\").agg(count(\"*\").alias(\"count\"), avg(\"delay\")).orderBy(\"count\", ascending=False).show()\n",
    "\n",
    "### Using trick with F import\n",
    "df_null.groupBy(\"origin\", \"destination\").agg(F.count(\"*\").alias(\"count\"), F.avg(\"delay\")).orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cd9358c-8db9-4ffa-a79e-db77611eb6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+----------+\n",
      "|   date|delay|distance|origin|destination|        id|\n",
      "+-------+-----+--------+------+-----------+----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|         0|\n",
      "|1020600|   -8|     369|   ABE|        DTW|         1|\n",
      "|1021245|   -2|     602|   ABE|        ATL|         2|\n",
      "|1020605|   -4|     602|   ABE|        ATL|         3|\n",
      "|1031245|   -4|     602|   ABE|        ATL|         4|\n",
      "|1030605|    0|     602|   ABE|        ATL|         5|\n",
      "|1041243|   10|     602|   ABE|        ATL|         6|\n",
      "|1040605|   28|     602|   ABE|        ATL|         7|\n",
      "|1051245|   88|     602|   ABE|        ATL|         8|\n",
      "|1050605|    9|     602|   ABE|        ATL|         9|\n",
      "|1011244|    5|       7|  NULL|       NULL|8589934592|\n",
      "|   NULL| NULL|    NULL|  NULL|       NULL|8589934593|\n",
      "+-------+-----+--------+------+-----------+----------+\n",
      "\n",
      "10\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "### Dealing with null values\n",
    "### Drop all rows where even one null\n",
    "df_null = df_null.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "df_null.show()\n",
    "print(df_null.na.drop().count())  # def how=any / =all where all columns null\n",
    "\n",
    "### Drop rows where at least 4 not None values\n",
    "print(df_null.na.drop(thresh=4).count())\n",
    "\n",
    "### Drop rows where None in specific column\n",
    "print(df_null.na.drop(subset=[\"date\"]).count())\n",
    "\n",
    "### Fill null values where string=None\n",
    "df_null.na.fill(\"-\")\n",
    "\n",
    "### Fill null values in specific cols\n",
    "df_null.na.fill(value=0, subset=[\"delay\"]).tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ae63346-0a7d-4458-ae7f-bf97886cc2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------+----------------+-----------+\n",
      "|  namespace|        viewName|isTemporary|\n",
      "+-----------+----------------+-----------+\n",
      "|global_temp|global_temp_view|       true|\n",
      "|           |       temp_view|       true|\n",
      "+-----------+----------------+-----------+\n",
      "\n",
      "+--------------+\n",
      "|     namespace|\n",
      "+--------------+\n",
      "|       default|\n",
      "|learn_spark_db|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### SQL\n",
    "### TEMPORARY views are session-scoped\n",
    "df_pyspark.createOrReplaceTempView(\"temp_view\")  # FROM temp_view\n",
    "df_pyspark.createOrReplaceGlobalTempView(\"global_temp_view\")  # FROM global_temp.global_temp_view\n",
    "\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay < 120 THEN 'Short Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM temp_view\n",
    " ORDER BY origin, delay DESC\"\"\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f454c9d5-6560-4342-ac78-fc25dee17c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----------+\n",
      "|  namespace|        viewName|isTemporary|\n",
      "+-----------+----------------+-----------+\n",
      "|global_temp|global_temp_view|       true|\n",
      "|           |       temp_view|       true|\n",
      "+-----------+----------------+-----------+\n",
      "\n",
      "+--------------+\n",
      "|     namespace|\n",
      "+--------------+\n",
      "|       default|\n",
      "|learn_spark_db|\n",
      "+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()\n",
    "spark.sql(\"SHOW VIEWS IN GLOBAL_TEMP\").show()\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.catalog.listColumns(\"temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bbe3449-9cb1-4c9c-b5f1-5c18d3d23d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Create and use DB\n",
    "# spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")\n",
    "\n",
    "### Managed table\n",
    "# spark.sql(\"CREATE TABLE managed_us_delay (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")\n",
    "\n",
    "### Same AS\n",
    "# df_small.write.saveAsTable(\"managed_us_delay1\")\n",
    "spark.sql(\"SELECT * FROM managed_us_delay1\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665c350-3eee-44cc-a993-52f9f5ec79c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Un-Managed\n",
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, distance INT, origin STRING, destination STRING) USING csv OPTIONS (PATH '/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')\"\"\")\n",
    "\n",
    "# SAME AS\n",
    "# (flights_df\n",
    "#  .write\n",
    "#  .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    "#  .saveAsTable(\"us_delay_flights_tbl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "87e56ec8-32d5-4c30-b1b2-81e84877c7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|cubed(cast(delay as int))|\n",
      "+-------------------------+\n",
      "|                      216|\n",
      "|                     -512|\n",
      "+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/08 16:16:26 WARN SimpleFunctionRegistry: The function cubed replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "+---+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### PySpark UDFs\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "@F.udf(returnType=LongType())\n",
    "def cubed(s):\n",
    "    if s is not None:\n",
    "        return s * s * s        \n",
    "# Generate temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")\n",
    "\n",
    "df_pyspark.select(cubed(col(\"delay\").cast(\"int\"))).show(2)\n",
    "\n",
    "# To use in SQL it has to be registered\n",
    "spark.udf.register(\"cubed\", cubed)\n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3573a867-9d99-47ca-b541-53542e495785",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|cubed2(cast(delay as int))|\n",
      "+--------------------------+\n",
      "|                       216|\n",
      "|                      -512|\n",
      "+--------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Pandas UDFs (vectorized user defined function)\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "@pandas_udf(LongType())\n",
    "def cubed2(a: pd.Series) -> pd.Series:\n",
    "   return a * a * a\n",
    "\n",
    "df_pyspark.select(cubed2(col(\"delay\").cast(\"int\"))).show(2)\n",
    "# cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d657f5f0-dfa2-4073-aee5-c44edcb706e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+--------------------+-------------+\n",
      "|   Name|Sorted_Numbers|slice(Numbers, 2, 3)|size(Numbers)|\n",
      "+-------+--------------+--------------------+-------------+\n",
      "|  Alice|     [2, 4, 6]|              [4, 6]|            3|\n",
      "|Charlie|     [4, 5, 6]|              [5, 6]|            3|\n",
      "+-------+--------------+--------------------+-------------+\n",
      "\n",
      "+-------+------+\n",
      "|   Name|Number|\n",
      "+-------+------+\n",
      "|  Alice|     2|\n",
      "|  Alice|     4|\n",
      "|  Alice|     6|\n",
      "|    Bob|     1|\n",
      "|    Bob|     2|\n",
      "|    Bob|     3|\n",
      "|Charlie|     4|\n",
      "|Charlie|     5|\n",
      "|Charlie|     6|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains, array_sort, array_union, array_intersect, slice,explode, size, map_keys\n",
    "data = [(\"Alice\", [2, 4, 6]), \n",
    "        (\"Bob\", [1, 2, 3]),\n",
    "        (\"Charlie\", [4, 5, 6])]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Numbers\"])\n",
    "\n",
    "df.filter(array_contains(df.Numbers, 4)).select(\"Name\", array_sort(df.Numbers).alias(\"Sorted_Numbers\"), slice(df.Numbers, 2, 3),\n",
    "                                               size(df.Numbers)).show()\n",
    "df.select(\"Name\", explode(df.Numbers).alias(\"Number\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ff93c5-091b-4054-814d-5ded92259c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------+\n",
      "|name   |properties                   |\n",
      "+-------+-----------------------------+\n",
      "|James  |{eye -> brown, hair -> black}|\n",
      "|Michael|{eye -> NULL, hair -> brown} |\n",
      "+-------+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+-----+-----+\n",
      "|      name| hair|  eye|\n",
      "+----------+-----+-----+\n",
      "|     James|black|brown|\n",
      "|   Michael|brown| NULL|\n",
      "|    Robert|  red|black|\n",
      "|Washington| grey| grey|\n",
      "| Jefferson|brown|     |\n",
      "+----------+-----+-----+\n",
      "\n",
      "+-----+----+-----+\n",
      "| name| key|value|\n",
      "+-----+----+-----+\n",
      "|James| eye|brown|\n",
      "|James|hair|black|\n",
      "+-----+----+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+--------------------+\n",
      "|   name|map_keys(properties)|\n",
      "+-------+--------------------+\n",
      "|  James|         [eye, hair]|\n",
      "|Michael|         [eye, hair]|\n",
      "+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, MapType\n",
    "schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('properties', MapType(StringType(),StringType()),True)\n",
    "])\n",
    "dataDictionary = [\n",
    "        ('James',{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',{'hair':'brown','eye':None}),\n",
    "        ('Robert',{'hair':'red','eye':'black'}),\n",
    "        ('Washington',{'hair':'grey','eye':'grey'}),\n",
    "        ('Jefferson',{'hair':'brown','eye':''})\n",
    "        ]\n",
    "df = spark.createDataFrame(data=dataDictionary, schema = schema)\n",
    "df.show(2, truncate=False)\n",
    "df.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n",
    "  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n",
    "  .drop(\"properties\") \\\n",
    "  .show()\n",
    "df.select(df.name,explode(df.properties)).show(2)\n",
    "\n",
    "df.select(df.name,map_keys(df.properties)).show(2) # map_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8d7e047-8ac9-43ab-ab88-d79132954c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448\n",
      "23310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### Cache\n",
    "# df2.count() is the first action hence it triggers the execution of reading a CSV file, and df.where().\n",
    "# df3.count(), this again triggers execution of reading a file - we are reading twice\n",
    "df=spark.read.csv('flight_delays/data_flights.csv',header=True,inferSchema=True)\n",
    "  \n",
    "df2 = df.where(col(\"origin\") ==\"ABE\")\n",
    "count = df2.count()\n",
    "print(count)\n",
    "\n",
    "df3 = df.where(col(\"destination\") == \"DTW\")\n",
    "count = df3.count()\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
